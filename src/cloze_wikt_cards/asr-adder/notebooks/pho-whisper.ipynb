{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How this should work\n",
    "\n",
    "1. Have a server that is ready to take in audio via REST to \n",
    "   1. Transcribe the words\n",
    "   2. create n-grams of the words\n",
    "   3. Looks it up if it is contained in wiktionary\n",
    "   4. creates new anki cards for each, filling with wikt data.\n",
    "2. On Client: Have GUI that can record audio, then send it to the server. Continuously updates a tsv file with the newly generated vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 10:32:10.712906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-21 10:32:11.530800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ducha/mambaforge/envs/ml_gpu/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"vinai/PhoWhisper-medium\", device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mtranscriber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           AutomaticSpeechRecognitionPipeline\n",
      "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.automatic_speech_recognition.AutomaticSpeechRecognitionPipeline object at 0x766fc850c670>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/ml_gpu/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Pipeline that aims at extracting spoken text contained within some audio.\n",
      "\n",
      "The input can be either a raw waveform or a audio file. In case of the audio file, ffmpeg should be installed for\n",
      "to support multiple audio formats\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> transcriber = pipeline(model=\"openai/whisper-base\")\n",
      ">>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")\n",
      "{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'}\n",
      "```\n",
      "\n",
      "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      "\n",
      "Arguments:\n",
      "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      "    feature_extractor ([`SequenceFeatureExtractor`]):\n",
      "        The feature extractor that will be used by the pipeline to encode waveform for the model.\n",
      "    tokenizer ([`PreTrainedTokenizer`]):\n",
      "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      "        [`PreTrainedTokenizer`].\n",
      "    decoder (`pyctcdecode.BeamSearchDecoderCTC`, *optional*):\n",
      "        [PyCTCDecode's\n",
      "        BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)\n",
      "        can be passed for language model boosted decoding. See [`Wav2Vec2ProcessorWithLM`] for more information.\n",
      "    chunk_length_s (`float`, *optional*, defaults to 0):\n",
      "        The input length for in each chunk. If `chunk_length_s = 0` then chunking is disabled (default).\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        For more information on how to effectively use `chunk_length_s`, please have a look at the [ASR chunking\n",
      "        blog post](https://huggingface.co/blog/asr-chunking).\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    stride_length_s (`float`, *optional*, defaults to `chunk_length_s / 6`):\n",
      "        The length of stride on the left and right of each chunk. Used only with `chunk_length_s > 0`. This enables\n",
      "        the model to *see* more context and infer letters better than without this context but the pipeline\n",
      "        discards the stride bits at the end to make the final reconstitution as perfect as possible.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        For more information on how to effectively use `stride_length_s`, please have a look at the [ASR chunking\n",
      "        blog post](https://huggingface.co/blog/asr-chunking).\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    framework (`str`, *optional*):\n",
      "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "        installed. If no framework is specified, will default to the one currently installed. If no framework is\n",
      "        specified and both frameworks are installed, will default to the framework of the `model`, or to PyTorch if\n",
      "        no model is provided.\n",
      "    device (Union[`int`, `torch.device`], *optional*):\n",
      "        Device ordinal for CPU/GPU supports. Setting this to `None` will leverage CPU, a positive will run the\n",
      "        model on the associated CUDA device id.\n",
      "    torch_dtype (Union[`int`, `torch.dtype`], *optional*):\n",
      "        The data-type (dtype) of the computation. Setting this to `None` will use float32 precision. Set to\n",
      "        `torch.float16` or `torch.bfloat16` to use half-precision in the respective dtypes.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\n",
      "documentation for more information.\n",
      "\n",
      "Args:\n",
      "    inputs (`np.ndarray` or `bytes` or `str` or `dict`):\n",
      "        The inputs is either :\n",
      "            - `str` that is either the filename of a local audio file, or a public URL address to download the\n",
      "              audio file. The file will be read at the correct sampling rate to get the waveform using\n",
      "              *ffmpeg*. This requires *ffmpeg* to be installed on the system.\n",
      "            - `bytes` it is supposed to be the content of an audio file and is interpreted by *ffmpeg* in the\n",
      "              same way.\n",
      "            - (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`)\n",
      "                Raw audio at the correct sampling rate (no further check will be done)\n",
      "            - `dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate` and let this\n",
      "              pipeline do the resampling. The dict must be in the format `{\"sampling_rate\": int, \"raw\":\n",
      "              np.array}` with optionally a `\"stride\": (left: int, right: int)` than can ask the pipeline to\n",
      "              treat the first `left` samples and last `right` samples to be ignored in decoding (but used at\n",
      "              inference to provide more context to the model). Only use `stride` with CTC models.\n",
      "    return_timestamps (*optional*, `str` or `bool`):\n",
      "        Only available for pure CTC models (Wav2Vec2, HuBERT, etc) and the Whisper model. Not available for\n",
      "        other sequence-to-sequence models.\n",
      "\n",
      "        For CTC models, timestamps can take one of two formats:\n",
      "            - `\"char\"`: the pipeline will return timestamps along the text for every character in the text. For\n",
      "                instance, if you get `[{\"text\": \"h\", \"timestamp\": (0.5, 0.6)}, {\"text\": \"i\", \"timestamp\": (0.7,\n",
      "                0.9)}]`, then it means the model predicts that the letter \"h\" was spoken after `0.5` and before\n",
      "                `0.6` seconds.\n",
      "            - `\"word\"`: the pipeline will return timestamps along the text for every word in the text. For\n",
      "                instance, if you get `[{\"text\": \"hi \", \"timestamp\": (0.5, 0.9)}, {\"text\": \"there\", \"timestamp\":\n",
      "                (1.0, 1.5)}]`, then it means the model predicts that the word \"hi\" was spoken after `0.5` and\n",
      "                before `0.9` seconds.\n",
      "\n",
      "        For the Whisper model, timestamps can take one of two formats:\n",
      "            - `\"word\"`: same as above for word-level CTC timestamps. Word-level timestamps are predicted\n",
      "                through the *dynamic-time warping (DTW)* algorithm, an approximation to word-level timestamps\n",
      "                by inspecting the cross-attention weights.\n",
      "            - `True`: the pipeline will return timestamps along the text for *segments* of words in the text.\n",
      "                For instance, if you get `[{\"text\": \" Hi there!\", \"timestamp\": (0.5, 1.5)}]`, then it means the\n",
      "                model predicts that the segment \"Hi there!\" was spoken after `0.5` and before `1.5` seconds.\n",
      "                Note that a segment of text refers to a sequence of one or more words, rather than individual\n",
      "                words as with word-level timestamps.\n",
      "    generate_kwargs (`dict`, *optional*):\n",
      "        The dictionary of ad-hoc parametrization of `generate_config` to be used for the generation call. For a\n",
      "        complete overview of generate, check the [following\n",
      "        guide](https://huggingface.co/docs/transformers/en/main_classes/text_generation).\n",
      "    max_new_tokens (`int`, *optional*):\n",
      "        The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
      "\n",
      "Return:\n",
      "    `Dict`: A dictionary with the following keys:\n",
      "        - **text** (`str`): The recognized text.\n",
      "        - **chunks** (*optional(, `List[Dict]`)\n",
      "            When using `return_timestamps`, the `chunks` will become a list containing all the various text\n",
      "            chunks identified by the model, *e.g.* `[{\"text\": \"hi \", \"timestamp\": (0.5, 0.9)}, {\"text\":\n",
      "            \"there\", \"timestamp\": (1.0, 1.5)}]`. The original full text can roughly be recovered by doing\n",
      "            `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`."
     ]
    }
   ],
   "source": [
    "transcriber?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducha/mambaforge/envs/ml_gpu/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output = transcriber(\n",
    "    \"/home/ducha/Workspace/python/cloze-wikt-cards/asr-adder/notebooks/sample.wav\"\n",
    ")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xin chào quý vị và các bạn.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducha/mambaforge/envs/ml_gpu/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=translate, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=translate.\n"
     ]
    }
   ],
   "source": [
    "output = transcriber(\n",
    "    \"/home/ducha/Workspace/python/cloze-wikt-cards/asr-adder/notebooks/sample.wav\",\n",
    "    generate_kwargs={\n",
    "        \"task\": \"translate\",\n",
    "        \"language\": \"<|vi|>\",  # Specify the source language, e.g., French\n",
    "    },\n",
    ")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xin chào quý vị và các bạn.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
